{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning: Homework 5 & 6\n",
    "\n",
    "**Nathan Inkawhich**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Nathan Inkawhich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:  Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and format word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')\n",
    "    \n",
    "# Decode file\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "# Extract English words\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embedddings = all_embeddings[english_word_indices]\n",
    "\n",
    "# Normalize Embeddings to unit circle\n",
    "norms = np.linalg.norm(english_embedddings, axis=1)\n",
    "normalized_embeddings = english_embedddings.astype('float32') / norms.astype('float32').reshape([-1, 1])\n",
    "\n",
    "# Create LUT\n",
    "index = {word: i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "def print_similarity(w1,w2):\n",
    "    try:\n",
    "        print('{0}\\t{1}\\t'.format(w1,w2), \\\n",
    "          similarity_score('{}'.format(w1), '{}'.format(w2)))\n",
    "    except:\n",
    "        print('One of the words is not in the dictionary.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0000001\n",
      "cat\tfeline\t 0.8199548\n",
      "cat\tdog\t 0.590724\n",
      "cat\tmoo\t 0.0039538303\n",
      "cat\tfreeze\t -0.030225191\n"
     ]
    }
   ],
   "source": [
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare movie dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1: Train an MLP off of the average word embedding to predict sentiment (as done in class) but optimize the network settings to maximize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "# Shuffle full dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Split full dataset into train/test splits\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configs\n",
    "num_hidden_L1 = 100\n",
    "num_hidden_L2 = 20\n",
    "learning_rate = .05\n",
    "num_epochs = 250\n",
    "\n",
    "# Clear all old tf graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 300]) # Word embedding size = 300\n",
    "y = tf.placeholder(tf.float32, [None, 1]) # Binary classification output: \"good\" or \"bad\"\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, num_hidden_L1, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, num_hidden_L2, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.690119 Acc 0.55\n",
      "Epoch 50 Loss 0.60457414 Acc 0.71\n",
      "Epoch 100 Loss 0.23702054 Acc 0.93\n",
      "Epoch 150 Loss 0.20047058 Acc 0.91\n",
      "Epoch 200 Loss 0.11904577 Acc 0.97\n",
      "Final test accuracy: 0.9537713\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2:  Train a RNN from the word embeddings to predict sentiment (as done in class) and optimize the network settings to maximize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))  \n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear old tf stuff\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Configs\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 50\n",
    "num_epochs = 1\n",
    "\n",
    "# Input placeholders\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Build RNN\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.6973456715211871 Acc 0.5231646095566507\n",
      "batch 199 Loss 0.6073786472962632 Acc 0.6580323704982015\n",
      "batch 299 Loss 0.4412638618766594 Acc 0.7960813802179973\n",
      "batch 399 Loss 0.37871248174390876 Acc 0.8394178290795372\n",
      "batch 499 Loss 0.4126086860810042 Acc 0.8151977302386081\n",
      "batch 599 Loss 0.42612175790679896 Acc 0.7904779924002064\n",
      "batch 699 Loss 0.3922986187270466 Acc 0.8279460272620374\n",
      "batch 799 Loss 0.38161134662699775 Acc 0.8336617848638171\n",
      "batch 899 Loss 0.3739993986912681 Acc 0.8160768972219207\n",
      "batch 999 Loss 0.3222696428076551 Acc 0.8648031316807501\n",
      "Epoch 0 Loss 0.3448121127542254 Acc 0.8502255011363107\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.8951841359773371\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3:  Encode each vocabulary word as a one-hot vector. Train an MLP on the average of the onehot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build one hot embedding functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(english_words):  150875\n",
      "english_embedddings.shape:  (150875, 300)\n",
      "onehot_embeddings.shape:  (150875, 150875)\n",
      "Size of index dict:  150875\n"
     ]
    }
   ],
   "source": [
    "print(\"len(english_words): \", len(english_words))\n",
    "print(\"english_embedddings.shape: \", english_embedddings.shape)\n",
    "\n",
    "# Build onehot encoding scheme with an identity matrix\n",
    "onehot_embeddings = np.identity(len(english_words),dtype=np.float32)\n",
    "print(\"onehot_embeddings.shape: \", onehot_embeddings.shape)\n",
    "#print(np.sum(onehot_embeddings,axis=0))\n",
    "#print(np.sum(onehot_embeddings,axis=1))\n",
    "\n",
    "# Create LUT\n",
    "index = {word: i for i, word in enumerate(english_words)}\n",
    "print(\"Size of index dict: \", len(index.keys()))\n",
    "\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 150875-dimensional one-hot representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example_onehot(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [onehot_embeddings[index[w]] for w in words if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/test datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n",
      "# train:  1000\n",
      "# test:  411\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "\n",
    "# Split full dataset into train/test splits\n",
    "random.shuffle(dataset)\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n",
    "\n",
    "print(\"# train: \", len(train))\n",
    "print(\"# test: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configs\n",
    "num_hidden_L1 = 100\n",
    "num_hidden_L2 = 20\n",
    "learning_rate = .05\n",
    "num_epochs = 50\n",
    "\n",
    "# Clear all old tf graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 150875]) # Word embedding size = 150875\n",
    "y = tf.placeholder(tf.float32, [None, 1]) # Binary classification output: \"good\" or \"bad\"\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, num_hidden_L1, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, num_hidden_L2, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "initialize_all = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.69034976 Acc 0.62\n",
      "Epoch 10 Loss 0.6914494 Acc 0.52\n",
      "Epoch 20 Loss 0.6822378 Acc 0.56\n",
      "Epoch 30 Loss 0.6873419 Acc 0.51\n",
      "Epoch 40 Loss 0.6727189 Acc 0.55\n",
      "Test Review Data Shape:  (411, 150875)\n",
      "Final test accuracy: 0.54257905\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "print(\"Test Review Data Shape: \",np.array(test_reviews).shape)\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.4:  Encode each vocabulary word as a one-hot vector. Train RNN on the one-hot encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train/test datasets for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset:  1411\n",
      "# train:  1058\n",
      "# test:  353\n"
     ]
    }
   ],
   "source": [
    "### Choose Dataset\n",
    "with open(\"Data/movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]  \n",
    "#with open(\"Data/movie-pang02.txt\", \"r\",encoding=enc) as f:\n",
    "#    dataset = [convert_line_to_example_onehot(l) for l in f.readlines()]\n",
    "\n",
    "print(\"Length of Dataset: \",len(dataset))\n",
    "\n",
    "# Split full dataset into train/test splits\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n",
    "\n",
    "print(\"# train: \", len(train))\n",
    "print(\"# test: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear old tf stuff\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Configs\n",
    "n_steps = None\n",
    "n_inputs = 150875\n",
    "n_neurons = 50\n",
    "num_epochs = 1\n",
    "\n",
    "# Input placeholders\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Build RNN\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train RNN on one-hot encoded movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.7078109203179662 Acc 0.5312151174752224\n",
      "batch 199 Loss 0.6752613271450638 Acc 0.5977265225273367\n",
      "batch 299 Loss 0.6445762745594524 Acc 0.6285593617972456\n",
      "batch 399 Loss 0.5944456793475554 Acc 0.6732697116427385\n",
      "batch 499 Loss 0.5965433553075045 Acc 0.6741323217162326\n",
      "batch 599 Loss 0.5690051064577933 Acc 0.6893543298962496\n",
      "batch 699 Loss 0.5372331624982251 Acc 0.751097568804388\n",
      "batch 799 Loss 0.5039331582726344 Acc 0.8341286004031883\n",
      "batch 899 Loss 0.34372292073487765 Acc 0.8899887649158932\n",
      "batch 999 Loss 0.3288086386782088 Acc 0.8999821138804149\n",
      "Epoch 0 Loss 0.34232794468014033 Acc 0.8771517269459184\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "l_ma=.74\n",
    "acc_ma=.5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,150875]) # New dims\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        l_ma=.99*l_ma+(.01)*l\n",
    "        acc_ma=.99*acc_ma+(.01)*acc\n",
    "        if (batch+1) % 100 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test trained RNN on one-hot encoded test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8781869688385269\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,150875])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final test accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.5: Why did the word embeddings work better (hint: the word embeddings will work better…)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.6: How does cross-validation change when considering a time-series instead of multiple instances (as in our movie reviews)? Only a description is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.7: In our previous homework assignment we considered the conditional GAN. In that case, the conditional label was known and given. Instead, consider generating images to match text. One approach could be to use an RNN to encode text to a vector that is fed to a conditional GAN (e.g. http://proceedings.mlr.press/v48/reed16.pdf). Draw a graph (but do not implement) how such a system could work. Any implementation here is completely optional, we are only looking for a description of how this could work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
